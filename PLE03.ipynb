{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PL03: Implementación del Bipedal Walker de OpenAI gymnasium\n",
    "\n",
    "### Práctica realizada por Víctor Vega Sobral\n",
    "\n",
    "El objetivo de esta práctica es entrenar un agente por aprendizaje por refuerzo, concretamente el modelo Bipedal Walker, para que sea capaz de caminar por terrenos con un relevo irregular.\n",
    "\n",
    "Para ello, se usará la [propia documentación de OpenAI Gymnasium](https://gymnasium.farama.org/environments/box2d/bipedal_walker/), junto a la [información disponible en una sección específica de la página Pylessons](https://pylessons.com/BipedalWalker-v3-PPO). Pylessons también tiene un [video en Youtube donde se explica lo presente en la página web](https://www.youtube.com/watch?v=2Epn__SRHns).\n",
    "\n",
    "Dichas fuentes contienen información muy útil tanto para la implementación del modelo como para su comprensión y explicación paso a paso. \n",
    "\n",
    "<img src=\"images/imagen_walker.jpg\" alt=\"Imagen Bipedal Walker\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importación de bibliotecas necesarias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias y enlaces de interés\n",
    "\n",
    "Además de los enlaces anteriores, a continuación se dejan otras referencias y enlaces que pueden resultar de interés.\n",
    "\n",
    "1. [Código fuente del modelo de Pylessons](https://github.com/pythonlessons/Reinforcement_Learning)\n",
    "2. [Repositorio de Github de la PLE03](https://github.com/VforVitorio/PLE03)\n",
    "3. [Video solucionando errores de instalación de Gymnasium en Windows](https://www.youtube.com/watch?v=gMgj4pSHLww&t=148s)\n",
    "4. [Repositorio con otras explicaciones útiles de este modelo](https://github.com/openai/gym/wiki/BipedalWalker-v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Victor Vega Sobral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros pasos\n",
    "\n",
    "### Validación de las bibliotecas\n",
    "\n",
    "Una vez instaladas las bibliotecas necesarias de requirements.txt, se procede a copiar el primer bloque de código que se encuentra en la documentación de Gymnasium. Este código servirá para validar que se renderiza el modelo y los componentes de la librería.\n",
    "\n",
    "También se ha creado un archivo \"validation.py\" donde se puede encontrar este mismo bloque.\n",
    "\n",
    "El código se deja como comentado, puesto que su ejecución supone la creación de una pantalla de Pygame y su posterior parada puede dar lugar a salidas de celdas con posibles errores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium as gym\n",
    "\n",
    "# # Initialise the environment\n",
    "# env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "# # Reset the environment to generate the first observation\n",
    "# observation, info = env.reset(seed=42)\n",
    "# for _ in range(1000):\n",
    "#     # this is where you would insert your policy\n",
    "#     action = env.action_space.sample()\n",
    "\n",
    "#     # step (transition) through the environment with the action\n",
    "#     # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "#     observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#     # If the episode has ended then we can reset to start a new episode\n",
    "#     if terminated or truncated:\n",
    "#         observation, info = env.reset()\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "Para mayor facilidad en la realización de la práctica y posterior comprensión de los conocimientos por parte de otras personas, se procede a la explicación de aspectos básicos de la librería y el aprendizaje por refuerzo. Todo lo explicado en este punto se encuentra traducido de la [explicación oficial de la biblioteca](https://gymnasium.farama.org/introduction/basic_usage/), siendo algunas de las partes resumidas. \n",
    "\n",
    "El uso de estas explicaciones se debe a la calidad de las explicaciones, además de resultar una fuente totalmente fiable de información, libre de posibles errores.\n",
    "\n",
    "#### Interacción con el entorno\n",
    "\n",
    "En el aprendizaje por refuerzo, el clásico \"bucle agente-entorno\", representado en la imagen a continuación, es una representación simplificada de cómo un agente y el entorno interactúan entre sí. \n",
    "\n",
    "El agente recibe una observación sobre el entorno, luego selecciona una acción que el entorno utiliza para determinar la recompensa y la siguiente observación. El ciclo se repite hasta que el entorno termina (se finaliza), como se puede observar en la siguiente imagen:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/flujo_funcionamiento.png\" alt=\"Imagen Bipedal Walker\" width=\"350\">\n",
    "</div>\n",
    "\n",
    "#### Explicación básica de algunos componentes del código\n",
    "\n",
    "Primero, se crea un entorno con `make()`, especificando el parámetro `render_mode`, que define cómo se visualizará el entorno [ver la documentación de `Env.render()` para más detalles.](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset) En este ejemplo, usamos el entorno **\"LunarLander\"**, donde el agente controla una nave que debe aterrizar de forma segura.\n",
    "\n",
    "Tras inicializar el entorno, usamos `env.reset()` para obtener la primera observación y datos adicionales. Si se desea inicializar el entorno con una semilla aleatoria o parámetros específicos, se pueden usar los argumentos `seed` u `options`.\n",
    "\n",
    "Para continuar el bucle **agente-entorno** hasta que el entorno termine (un número desconocido de pasos), definimos la variable `episode_over` para indicar cuándo detenernos y usamos un bucle `while`.\n",
    "\n",
    "El agente realiza una acción con `env.step()`, que ejecuta la acción seleccionada (en este caso, aleatoria con `env.action_space.sample()`). Esto simula una interacción, como mover un robot o presionar un botón, lo que provoca un cambio en el entorno. Como resultado, el agente recibe:\n",
    "1. Una nueva observación del entorno actualizado.\n",
    "2. Una recompensa asociada a la acción (positiva o negativa).\n",
    "\n",
    "El entorno puede terminar después de algunos pasos, lo que se llama un **estado terminal** (por ejemplo, si el robot se estrella o completa su tarea). `env.step()` devuelve una señal `terminated` cuando esto ocurre. También podemos definir un límite de tiempo para finalizar el episodio, lo que activa la señal `truncated`. Si cualquiera de estas señales (`terminated` o `truncated`) es `True`, el episodio termina. En ese caso, podemos reiniciar el entorno con `env.reset()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipedal Walker\n",
    "\n",
    "A partir de este punto, **comienza en si lo referente a la implementación del Bipedal Walker**. Su importación se hace mediante la línea de código `gymnasium.make(\"BipedalWalker-v3\")`. \n",
    "\n",
    "En esta celda introductoria de Markdown, se encuentran tanto explicaciones adaptadas como propias del estudiante, junto con algunas tablas e imágenes que refuerzan el entendimiento del modelo.\n",
    "\n",
    "### Descripción\n",
    "\n",
    "Se trata de un entorno de un robot con 4 articulaciones, habiendo dos versiones del entorno:\n",
    "1. Normal, con un terreno levemente irregular.\n",
    "2. Hardcore, con escaleras y otro tipo de obstáculos.\n",
    "\n",
    "Según la documentación, para resolver la versión normal, se requieren de 300 puntos en 1600 intervalos de tiempo. Para la versión más complicadas, el número asciende a 2000 intervalos de tiempo.\n",
    "\n",
    "Cabe destacar que estos números presuntamente solo han sido conseguidos por unas 10 personas debida su complejidad, según lo comentado por Pylessons. Por tanto, no se espera que el rendimiento del modelo sea tan óptimo, puesto que ni el modelo propuesto en esta página alcanza estas marcas.\n",
    "\n",
    "\n",
    "#### Espacio de acción\n",
    "\n",
    "Se tratan de velocidades de motor en el intervalo [-1, 1] para cada una de las 4 articulaciones en las caderas y rodillas.\n",
    "\n",
    "| Num | Nombre                                | Mínimo | Máximo |\n",
    "|-----|---------------------------------------|--------|--------|\n",
    "| 0   | Hip_1 (Torque / Velocidad)            | -1     | +1     |\n",
    "| 1   | Knee_1 (Torque / Velocidad)          | -1     | +1     |\n",
    "| 2   | Hip_2 (Torque / Velocidad)            | -1     | +1     |\n",
    "| 3   | Knee_2 (Torque / Velocidad)          | -1     | +1     |\n",
    "\n",
    "\n",
    "#### Espacio de observación\n",
    "\n",
    "El estado consiste en diferentes variables, como velocidad angular, horizontal, vertical, posición de las articulaciones, el contacto de las piernas con el suelo y 10 mediciones del rango de los sensores lidar.\n",
    "\n",
    "| Num | Observación                          | Mínimo | Máximo   | Media |\n",
    "|-----|---------------------------------------|--------|----------|-------|\n",
    "| 0   | hull_angle                           | 0      | 2*pi     | 0.5   |\n",
    "| 1   | hull_angularVelocity                 | -inf   | +inf     | -     |\n",
    "| 2   | vel_x                                 | -1     | +1       | -     |\n",
    "| 3   | vel_y                                 | -1     | +1       | -     |\n",
    "| 4   | hip_joint_1_angle                     | -inf   | +inf     | -     |\n",
    "| 5   | hip_joint_1_speed                     | -inf   | +inf     | -     |\n",
    "| 6   | knee_joint_1_angle                    | -inf   | +inf     | -     |\n",
    "| 7   | knee_joint_1_speed                    | -inf   | +inf     | -     |\n",
    "| 8   | leg_1_ground_contact_flag            | 0      | 1        | -     |\n",
    "| 9   | hip_joint_2_angle                     | -inf   | +inf     | -     |\n",
    "| 10  | hip_joint_2_speed                     | -inf   | +inf     | -     |\n",
    "| 11  | knee_joint_2_angle                    | -inf   | +inf     | -     |\n",
    "| 12  | knee_joint_2_speed                    | -inf   | +inf     | -     |\n",
    "| 13  | leg_2_ground_contact_flag            | 0      | 1        | -     |\n",
    "| 14-23 | 10 mediciones lidar                  | -inf   | +inf     | -     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias para caminar \n",
    "\n",
    "Existen 4 tipos de estrategias al caminar:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/estrategias_walker.png\" alt=\"Imagen Bipedal Walker\" width=\"350\">\n",
    "</div>\n",
    "\n",
    "1. **Balance con las rodillas**: las rodillas del robot se mantienen alineadas para proporcionar un control eficiente y estabilidad mientras camina. Se trata de la estrategia óptima para enseñar al modelo a caminar en un entorno simulado, ya que proporciona un punto de equilibrio óptimo entre la velocidad y el control.\n",
    "\n",
    "2. **Balance doble**: el Walker usa ambas piernas de forma más activa, resultando en un movimiento mucho más rápido, sacrificando estabilidad y control y provocando caídas más frecuentes. No obstante, es una estrategia útil para enseñar al caminante su máxima velocidad.\n",
    "\n",
    "3. **Balance frontal**: pone énfasis en mantener la parte delantera del cuerpo equilibrada, siendo útil en posibles entornos inclinados o moverse hacia adelante más rápido en combinación con la estrategia anterior. Se trata, junto a la siguiente estrategia, en una de las más útiles para adaptarse a la versión Hardcore del modelo.\n",
    "\n",
    "4. **Balance trasero**: similar al balance frontal, pero manteniendo más atención a la parte trasera. Es importante para que mantenga estabilidad en entornos donde se realicen otros movimientos más dinámicos y rápidos. \n",
    "\n",
    "### Entrenamiento a seguir\n",
    "\n",
    "Se realizará un entrenamiento por fases, donde primero se intentará conseguir una estabilización del agente en el entorno con el balance de rodillas, para posteriormente aplicar los otros 3 tipos de estrategias para que comience a moverse. No obstante, en la ejecución del modelo como tal para caminar, se combinan las 4 estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Antes de seguir\n",
    "\n",
    "No obstante, antes de seguir con la práctica, se dispone de un código de prueba para visualizar la interacción de un agente Walker con el entorno y comprobar su correcta instalación y funcionamiento. \n",
    "\n",
    "El código correspondiente se encuentra en el archivo `validation_walker.py`, siendo este el archivo oficial de OpenAI gymnasium. Dado que no se especifica en la documentación como ejecutarlo, para extraer el código se debe navegar a los envs que se instalan al instalar la propia librería y allí se encuentra el código fuente de esta demo.\n",
    "\n",
    "Los resultados arrojados por terminal durante la ejecución son los siguientes son los siguientes:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/captura_terminal.png\" alt=\"Imagen Bipedal Walker\" width=\"350\">\n",
    "</div>\n",
    "\n",
    "El entorno BipedalWalker utiliza un espacio de acción continuo, donde los valores que controlan las articulaciones del robot (hull, legs) varían entre pequeños rangos. \n",
    "\n",
    "En la captura de la terminal de la demp, podemos ver cómo las articulaciones del robot, como el cuerpo (hull) y las piernas (leg0 y leg1), toman valores precisos en cada paso, con oscilaciones mínimas pero consistentes. Los valores oscilan entre pequeñas variaciones en cada eje, como se observa en los pasos 60 y 80, con recompensas negativas y positivas, respectivamente.\n",
    "\n",
    "Estos números, aunque no tengan mucho sentido para un observador humano, son fundamentales para el agente de inteligencia artificial que controla al robot. Dado que el espacio de acción es continuo, hay miles de posibles valores entre -1 y 1 que el agente puede explorar para decidir la acción óptima en cada momento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Se procede al entrenamiento del modelo, siguiendo la estrategia definida anteriormente. Para ello, se usará el algoritmo de POO de aprendizaje por refuerzo. \n",
    "\n",
    "### Componentes de PPO\n",
    "\n",
    "- **Política**: La política (denotada como π) es el conjunto de reglas que el agente sigue para decidir qué acciones tomar. En PPO, esta política es representada por una red neuronal.\n",
    "\n",
    "- **Función de valor:** PPO también usa una función de valor (denotada como V) que estima el valor esperado de la recompensa futura que el agente puede obtener a partir de un estado dado. Esto ayuda a optimizar las decisiones a largo plazo.\n",
    "\n",
    "- **Función objetivo:** El objetivo de PPO es maximizar la recompensa que el agente obtiene a lo largo de su vida útil. Sin embargo, PPO introduce una restricción para garantizar que la política no cambie demasiado de un paso a otro, lo que podría causar inestabilidad en el entrenamiento. Se trata de algo perfecto para nuestro Walker, puesto que limitará saltos muy grandes en los valores de las articulaciones que pueden resultar contraproducentes.\n",
    "\n",
    "- **Optimización:** PPO optimiza una función objetivo que permite hacer pequeñas actualizaciones a la política. La actualización de la política se limita mediante un \"clip\" en la función de pérdida, lo que asegura que la política no cambie demasiado entre iteraciones sucesivas.\n",
    "\n",
    "### Funcionamiento de PPO\n",
    "- **Experiencia en lotes:** PPO utiliza lotes de experiencia para calcular las políticas y actualizar la red neuronal. La experiencia se recopila ejecutando el agente en el entorno durante un número determinado de pasos.\n",
    "\n",
    "- **Estimación de ventajas:** A partir de la experiencia acumulada, PPO calcula las ventajas de las acciones tomadas, que son una medida de qué tan buena fue una acción comparada con otras posibles en ese estado. \n",
    "\n",
    "- **Actualización de la política:** PPO usa un algoritmo de optimización por gradientes para actualizar los parámetros de la red neuronal de la política. Esto se realiza usando el gradiente descendente, pero se limita la magnitud del cambio en cada paso de optimización usando el método de \"clipping\".\n",
    "\n",
    "- **Clipping**: La técnica de \"clipping\" se introduce para evitar que el cambio de política sea demasiado grande entre actualizaciones. Si la relación de probabilidad entre la política actual y la anterior es demasiado grande, PPO limita este cambio para evitar que el agente haga un gran salto en la política, lo que puede ser contraproducente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación del tipo de entrenamiento\n",
    "\n",
    "Junto con POO, para seguir la estrategia definida de un balance estático y un posterior balance dinámico, me remitiré a implementar un **algoritmo Actor Critic**, que combinan el valor y la política. Las imágenes explicativas se pueden encontrar en la explicación dada por Pylessons.\n",
    "\n",
    "1. **Actor**.\n",
    "    - Responsables de tomar decisiones sobre qué acción ejecutar dado el estado del entorno. Aquí se define la política del agente.\n",
    "    - El actor toma el estado del entorno y selecciona una acción basada en la poítica. Es decir, usa la información del entorno para ejecutar una acción.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/actor_model.png\" alt=\"Imagen Bipedal Walker\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "2. **Crítico**.\n",
    "    - Evalúa las acciones del actor, estimando el valor de las acciones tomadas y evaluando la recompensa a largo plazo que se podrá obtener desde el estado actual.\n",
    "    - Calcula el valor de la función de valor, como el Q-value, permitiendo dar una retroalimentación al actor sobre la calidad de decisiones que está tomando para mejorar la política.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/critic_model.png\" alt=\"Imagen Bipedal Walker\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "#### Caso personal\n",
    "\n",
    "Debido a que se quiere pasar de un entrenamiento estático a uno dimámico, el enfoque será el del algoritmo previamente dicho: \n",
    "\n",
    "1. **Estabilización estática:** En este paso, el objetivo principal es entrenar al agente para que logre caminar sin caerse, lo cual implica entrenar la política de manera que se logre un comportamiento estable. El actor estaría seleccionando acciones para lograr este equilibrio, mientras que el crítico proporcionaría una retroalimentación sobre qué tan bien se están haciendo esas acciones para mantenerse equilibrado.\n",
    "\n",
    "\n",
    "\n",
    "2. **Optimización dinámica (velocidad):** Una vez que el agente ha aprendido a mantenerse estable, el siguiente paso sería optimizar su rendimiento, buscando no solo estabilidad, sino también velocidad. En este caso, el actor seguiría eligiendo acciones para maximizar la velocidad, mientras que el crítico continuaría evaluando esas acciones con base en las recompensas obtenidas (en este caso, acelerando el movimiento sin perder el equilibrio).\n",
    "\n",
    "La siguiente imagen creada por el autor de Pylessons resume a la perfección la estructura de este tipo de modelo: \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/actor_critic_model.png\" alt=\"Imagen Bipedal Walker\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo del código\n",
    "\n",
    "A partir de este punto, se procede a hacer un desarrollo de un código independiente al de las fuentes consultadas. Lo que se mantiene es la estructura del código, con una clase actor y otra agent para entrenar el modelo, aplicando los cambios específicos para un modelo continuo y no discreto.\n",
    "\n",
    "Para facilitar el desarrollo y tras esta explicación, se usarán librerías como Keras o Tensorflow que puedan simplificar el desarrollo del modelo, sin partir de una base scracth, puesto que dicha complejidad sería inabarcable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase actor-critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(tf.keras.Model):\n",
    "    def __init__(self, action_dim, hidden_units=(256, 256)):\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "        \n",
    "        # Simplified network architecture\n",
    "        self.shared_layers = [\n",
    "            layers.Dense(hidden_units[0], activation='relu'),\n",
    "            layers.Dense(hidden_units[1], activation='relu')\n",
    "        ]\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor_output = layers.Dense(action_dim, activation='tanh')\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic_output = layers.Dense(1)\n",
    "    \n",
    "    def call(self, state):\n",
    "        # Shared layers\n",
    "        x = state\n",
    "        for layer in self.shared_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Actor and critic specific outputs\n",
    "        action_mean = self.actor_output(x)\n",
    "        value = self.critic_output(x)\n",
    "        \n",
    "        return action_mean, value\n",
    "    \n",
    "    def get_action(self, state, noise_scale=0.1):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        action_mean, _ = self(state)\n",
    "        \n",
    "        # Simplified action sampling\n",
    "        action = action_mean[0].numpy() + np.random.normal(0, noise_scale, size=action_mean.shape[1])\n",
    "        \n",
    "        # Clip action to valid range [-1, 1]\n",
    "        return np.clip(action, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clase agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipedalWalkerAgent:\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Actor-Critic Network\n",
    "        self.network = ActorCriticNetwork(action_dim)\n",
    "        \n",
    "        # Compile the network\n",
    "        self.network.compile(optimizer='adam')\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99  # Discount factor\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def train(self, states, actions, rewards, next_states, dones):\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Compute current and next values\n",
    "            _, current_values = self.network(states)\n",
    "            _, next_values = self.network(next_states)\n",
    "            \n",
    "            # Compute TD target\n",
    "            targets = rewards + self.gamma * next_values.numpy() * (1 - dones)\n",
    "            \n",
    "            # Compute combined loss\n",
    "            value_loss = tf.reduce_mean(tf.square(targets - current_values))\n",
    "            policy_loss = -tf.reduce_mean(value_loss)\n",
    "            total_loss = value_loss + policy_loss\n",
    "        \n",
    "        # Compute gradients and apply\n",
    "        gradients = tape.gradient(total_loss, self.network.trainable_variables)\n",
    "        self.network.optimizer.apply_gradients(zip(gradients, self.network.trainable_variables))\n",
    "        \n",
    "        return total_loss.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo de gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bipedal_walker(episodes=100, max_steps=800, render=False):\n",
    "    # Initialize environment\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    \n",
    "    # Get dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    # Create agent\n",
    "    agent = BipedalWalkerAgent(state_dim, action_dim)\n",
    "    \n",
    "    # Training logs\n",
    "    episode_rewards = []\n",
    "    best_reward = float('-inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        # Lists to store episode data\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Get action from agent\n",
    "            action = agent.network.get_action(state)\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(terminated or truncated)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Check if episode is done\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Train agent on episode data\n",
    "        if len(states) > 1:\n",
    "            loss = agent.train(\n",
    "                np.array(states), \n",
    "                np.array(actions), \n",
    "                np.array(rewards), \n",
    "                np.array(next_states), \n",
    "                np.array(dones)\n",
    "            )\n",
    "        \n",
    "        # Log episode results\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Save best model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            best_model = agent.network\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    return episode_rewards, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13268\\2490820994.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_bipedal_walker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Plot rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13268\\3145975031.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(episodes, max_steps, render)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# Train agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# Update state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13268\\842184749.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Combine losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcritic_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpolicy_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# Compute and apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1062\u001b[0m               output_gradients))\n\u001b[0;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[0;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     raise ValueError(\n\u001b[0;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[0mgy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m     \u001b[0mgy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_ReduceGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, gx, gy)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_ReduceGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;34m\"\"\"Reduces gradients of both arguments of a broadcasting binary op.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mbx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSmartBroadcastGradientArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mgx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[0mgy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ReduceGradientArg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(grad, shape_axes_must_reduce)\u001b[0m\n\u001b[0;32m    130\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust_reduce\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m# Applying keepdims=True in presence of unknown axes opens up some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m# opportunities for optimizations. For example, _SumGrad below won't have to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# emit extra ops to recover reduced indices for broadcasting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2232\u001b[0m   \u001b[0mint64\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2233\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2234\u001b[0m   \"\"\"\n\u001b[0;32m   2235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2236\u001b[1;33m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0m\u001b[0;32m   2237\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   2244\u001b[0m                          dims=None):\n\u001b[0;32m   2245\u001b[0m   \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2246\u001b[0m   return _may_reduce_to_scalar(\n\u001b[0;32m   2247\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2248\u001b[1;33m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m",
      "\u001b[1;32md:\\Users\\O M E N\\miniconda3\\envs\\aa_env\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  13748\u001b[0m         _ctx, \"Sum\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[0;32m  13749\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13750\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13751\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 13752\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  13753\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13754\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  13755\u001b[0m       return _sum_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "rewards = train_bipedal_walker(episodes=500, render=False)\n",
    "\n",
    "# Plot rewards\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards)\n",
    "plt.title('Bipedal Walker Training Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bipedal_walker(model=None, episodes=3):\n",
    "    # Create environment with rendering\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='human')\n",
    "    \n",
    "    # If no model provided, train a quick model\n",
    "    if model is None:\n",
    "        _, model = train_bipedal_walker(episodes=50)\n",
    "    \n",
    "    # Reset random seed for consistency\n",
    "    env.reset(seed=42)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(2000):\n",
    "            # Get action from model\n",
    "            action = model.get_action(state)\n",
    "            \n",
    "            # Step environment\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if done\n",
    "            if terminated or truncated:\n",
    "                print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "                break\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model function\n",
    "def save_best_model(model, filename='best_bipedal_walker_model'):\n",
    "    # Ensure the model is not None\n",
    "    if model is not None:\n",
    "        # Save weights\n",
    "        model.save_weights(f'{filename}.weights.h5')\n",
    "        print(f\"Model saved to {filename}.weights.h5\")\n",
    "    else:\n",
    "        print(\"No model to save!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
